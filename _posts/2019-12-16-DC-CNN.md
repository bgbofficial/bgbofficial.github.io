# 1. Network flow

a）在训练中的调用

```python
err = train_fn(im_und, mask, k_und, im_gnd)[0]
train_err += err
```

b）创建实体

```python
# Compile function
train_fn, val_fn = compile_fn(net, net_config, args)
```

c）创建 DC-CNN 网络对象

```python
# Specify network
input_shape = (batch_size, 2, Nx, Ny)
net_config, net,  = build_d2_c2(input_shape)
```

d）创建d2_c2网络

第一个函数 `def build_d2_c2` ：

```python
def build_d2_c2(shape):
    def cascade_d2(pr, net, input_layer, **kwargs):
        return cascade_resnet(pr, net, input_layer, n=2)
    return build_cascade_cnn_from_list(shape, [(cascade_d2, 2)])
```

第二个函数 `cascade_resnet`：

```python
def cascade_resnet(pr, net, input_layer, n=5, nf=64, b=lasagne.init.Constant, **kwargs):
    shape = lasagne.layers.get_output_shape(input_layer)
    n_channel = shape[1]
    net[pr+'conv1'] = l.Conv(input_layer, nf, 3, b=b(), name=pr+'conv1')

    for i in range(2, n):
        net[pr+'conv%d'%i] = l.Conv(net[pr+'conv%d'%(i-1)], nf, 3, b=b(),
                                    name=pr+'conv%d'%i)

    net[pr+'conv_aggr'] = l.ConvAggr(net[pr+'conv%d'%(n-1)], n_channel, 3,
                                     b=b(), name=pr+'conv_aggr')
    net[pr+'res'] = l.ResidualLayer([net[pr+'conv_aggr'], input_layer],
                                    name=pr+'res')
    output_layer = net[pr+'res']
    return net, output_layer
```

第三个函数 `build_cascade_cnn_from_list`：

```python
def build_cascade_cnn_from_list(shape, net_meta, lmda=None):
    """
    Create iterative network with more flexibility

    net_meta: [(model1, cascade1_n),(model2, cascade2_n),....(modelm, cascadem_n),]
    """
    if not net_meta:
        raise

    net = OrderedDict()
    input_layer, kspace_input_layer, mask_layer = l.get_dc_input_layers(shape)
    net['input'] = input_layer
    net['kspace_input'] = kspace_input_layer
    net['mask'] = mask_layer

    j = 0
    for cascade_net, cascade_n in net_meta:
        # Cascade layer
        for i in range(cascade_n):
            pr = 'c%d_' % j
            net, output_layer = cascade_net(pr, net, input_layer,
                                            **{'cascade_i': j})

            # add data consistency layer
            net[pr+'dc'] = l.DCLayer([output_layer,
                                      net['mask'],
                                      net['kspace_input']],
                                     shape,
                                     inv_noise_level=lmda)
            input_layer = net[pr+'dc']
            j += 1

    output_layer = input_layer
    return net, output_layer
```

第四个函数 `def Conv`：

```PYTHON
if theano.config.device == 'cuda':
    from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer
    # MaxPool2DDNNLayer as MaxPool2DLayer
else:
    from lasagne.layers import Conv2DLayer as ConvLayer
    
def Conv(incoming, num_filters, filter_size=3,
         stride=(1, 1), pad='same', W=lasagne.init.HeNormal(),
         b=None, nonlinearity=lasagne.nonlinearities.rectify, **kwargs):
    """
    Overrides the default parameters for ConvLayer
    """
    ensure_set_name('conv', kwargs)

    return ConvLayer(incoming, num_filters, filter_size, stride, pad, W=W, b=b,
                     nonlinearity=nonlinearity, **kwargs)
```

第一个类 `ConvAggr`：

```python
from lasagne.layers import Layer, prelu

# Aggr = aggregate ?
class ConvAggr(Layer):
    def __init__(self, incoming, num_channels, filter_size=3, stride=(1, 1),
                 pad='same', W=lasagne.init.HeNormal(), b=None, **kwargs):
        ensure_set_name('conv_aggr', kwargs)
        super(ConvAggr, self).__init__(incoming, **kwargs)
        self.conv = Conv(incoming, num_channels, filter_size, stride, pad=pad,
                         W=W, b=b, nonlinearity=None, **kwargs)

        # copy params
        self.params = self.conv.params.copy()

    def get_output_for(self, input, **kwargs):
        return self.conv.get_output_for(input)

    def get_output_shape_for(self, input_shape):
        return self.conv.get_output_shape_for(input_shape)
```

第二个类 `ResidualLayer`：

```python
class ResidualLayer(lasagne.layers.ElemwiseSumLayer):
    '''
    Residual Layer, which just wraps around ElemwiseSumLayer
    '''

    def __init__(self, incomings, **kwargs):
        ensure_set_name('res', kwargs)
        super(ResidualLayer, self).__init__(incomings, **kwargs)
        # store names
        input_names = []
        for l in incomings:
            if isinstance(l, lasagne.layers.InputLayer):
                input_names.append(l.name if l.name else l.input_var.name)
            elif l.name:
                input_names.append(l.name)
            else:
                input_names.append(str(l))

        self.input_names = input_names

    def get_output_for(self, inputs, **kwargs):
        return super(lasagne.layers.ElemwiseSumLayer,
                     self).get_output_for(inputs, **kwargs)
```

第三个类 `DCLayer`：

```python
class DCLayer(MergeLayer):
    '''
    Data consistency layer
    '''
    def __init__(self, incomings, data_shape, inv_noise_level=None, **kwargs):
        if 'name' not in kwargs:
            kwargs['name'] = 'dc'

        super(DCLayer, self).__init__(incomings, **kwargs)
        self.inv_noise_level = inv_noise_level
        data, mask, sampled = incomings
        self.data = data
        self.mask = mask
        self.sampled = sampled
        self.dft2 = FFT2Layer(data, data_shape, name='dc_dft2')
        self.dc = DataConsistencyWithMaskLayer([self.dft2, mask, sampled],
                                               name='dc_consistency')
        self.idft2 = FFT2Layer(self.dc, data_shape, inv=True, name='dc_idft2')

    def get_output_for(self, inputs, **kwargs):
        x = inputs[0]
        mask = inputs[1]
        x_sampled = inputs[2]
        return get_output(self.idft2,
                          {self.data: x,
                           self.mask: mask,
                           self.sampled: x_sampled})

    def get_output_shape_for(self, input_shapes, **kwargs):
        return input_shapes[0]
```

第四个类 `DtaConsistencyWithMaskLayer`：

```python
class DataConsistencyWithMaskLayer(MergeLayer):
    '''
    Data consistency layer
    '''

    def __init__(self, incomings, inv_noise_level=None, **kwargs):
        super(DataConsistencyWithMaskLayer, self).__init__(incomings, **kwargs)
        self.inv_noise_level = inv_noise_level

    def get_output_for(self, inputs, **kwargs):
        '''

        Parameters
        ------------------------------
        inputs: 3 4d tensors
            First is data, second is the mask, third is the k-space samples

        Returns
        ------------------------------
        output: 4d tensor, data input with entries replaced with the sampled
        values
        '''
        x = inputs[0]
        mask = inputs[1]
        x_sampled = inputs[2]
        v = self.inv_noise_level
        if v:  # noisy case
            # out = (x_cnn + lambda * kspace_sampled) / (1 + lambda)
            out = (x + v * x_sampled) / (1 + v)
        else:  # noiseless case
            out = (1 - mask) * x + x_sampled
        return out
```

网络架构

```python
# Input layer
net['input'] = input_layer
net['kspace_input'] = kspace_input_layer
net['mask'] = mask_layer

# Component--cascade_resnet
'''
def cascade_resnet(pr, net, input_layer, 
	n=5, nf=64, b=lasagne.init.Constant, **kwargs):
return net, output_layer
'''
net[pr+'conv1']  # [input_layer, nf, 3, b=b()]  first conv layer
for i in range(2, n=5):  # [net[pr+'conv'%(i-1)], nf, 3, b=b()]  multiple conv layer
	net[pr+'conv%d'%i]
net[pr+'conv_aggr']  # [net[pr+'conv%d'%(n-1)], n_channel, 3, b=b()]  ConvAggr
net[pr+'res']  # [add([net[pr+'conv_aggr'], input_layer])]  ResidualLayer
output_layer = net[pr+'res']

# data consistency layer
dft2 = FFT2DLayer(output_layer)
dc = DataConsistencyWithMaskLayer(dft2, mask, sampled)
idft2 = FFT2DLayer(dc, inv=True)

# output layer
output_rec = idft2


# build DC-CNN
'''
def build_d2_c2(shape):
    def cascade_d2(pr, net, input_layer, **kwargs):
        return cascade_resnet(pr, net, input_layer, n=2)
    return build_cascade_cnn_from_list(shape, [(cascade_d2, 2)])
'''

```



# 2. Formula

By taking the same approach, for our CNN formulation, we enforce  $\mathbf{x}$  to be well-approximated by the CNN reconstruction (4):
$$
\begin{aligned} \begin{aligned}&\underset{\mathbf {x}, {\theta }}{\text {min.}}&\Vert \mathbf {x}- f_{\text {cnn}}(\mathbf {x}_u | {\theta }) \Vert ^2_2 + \lambda \Vert \mathbf {F}_u \mathbf {x}- \mathbf {y}\Vert _2^2 \\ \end{aligned} \end{aligned}
$$
In order to incorporate the data fidelity in the network architecture, we first note the following: for a fixed $θ$, Eq. ([4](https://link.springer.com/chapter/10.1007/978-3-319-59050-9_51#Equ4)) has a closed-form solution in *k*-space, given as in [[17](https://link.springer.com/chapter/10.1007/978-3-319-59050-9_51#CR17)]: (6)
$$
\begin{aligned}
\hat{\mathbf {x}}_{\text {rec}}(k) = {\left\{ \begin{array}{ll} \hat{\mathbf {x}}_{\text {cnn}}(k) &{} 
\text {if } k \not \in \varOmega \\ 
\frac{\hat{\mathbf {x}}_{\text {cnn}}(k) + \lambda \hat{\mathbf {x}}_u(k)}{1+\lambda } &{} 
\text {if } k \in \varOmega \end{array}\right. } 
\end{aligned}
$$
where $\hat{\mathbf {x}}_{\text {cnn}} = \mathbf {F} f_{\text {cnn}}(\mathbf {x}_u | {\theta })$ , $\hat{\mathbf {x}}_u = \mathbf {F}\mathbf {x}_u$ and $\mathbf {F}$  is the Fourier encoding matrix.

`x_rec = (x_cnnk + lambda * kspace_unsampled) / (1 + lambda)`

# 3. Data

```python
data.shape → (128, 128, 128, 2)
nx, ny, nz, nc = data.shape
```

(nx, ny, nz)是一张三维图。其中，nx 是体素中的垂直方向；(ny, nz) 是一张切片图；nc = 2 是有两张三维图。

